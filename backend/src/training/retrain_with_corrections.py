"""
–ü—Ä–æ—Å—Ç–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
"""
import pandas as pd
import json
import os
from pathlib import Path
from config import Config
from training.processed import preprocess_data, save_preprocessing_objects
from models.autoencoder_model import AutoencoderDL
from keras.utils import to_categorical

FEEDBACK_FILE = "src/data/feedback_corrections.json"

def load_corrections(marketplace: str):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞"""
    file_path = Path(FEEDBACK_FILE)
    if not file_path.exists():
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        all_corrections = json.load(f)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—É –∏ –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–º
    corrections = [
        c for c in all_corrections 
        if c.get('marketplace') == marketplace and not c.get('used_for_training', False)
    ]
    
    return corrections

def add_corrections_to_dataset(corrections, marketplace: str):
    """
    –î–æ–±–∞–≤–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –¥–∞—Ç–∞—Å–µ—Ç
    
    Args:
        corrections: —Å–ø–∏—Å–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        marketplace: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞
    
    Returns:
        DataFrame —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏
    """
    if not corrections:
        return pd.DataFrame()
    
    data = []
    for corr in corrections:
        data.append({
            'sku': f"correction_{corr['id']}",
            'product_name': corr['product_name'],
            'category_id': 0,  # –í—Ä–µ–º–µ–Ω–Ω—ã–π ID
            'category_name': corr['corrected_category'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            'category_path': corr['corrected_category']  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        })
    
    return pd.DataFrame(data)

def mark_corrections_as_used(marketplace: str):
    """–ü–æ–º–µ—Ç–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ"""
    file_path = Path(FEEDBACK_FILE)
    if not file_path.exists():
        return
    
    with open(file_path, 'r', encoding='utf-8') as f:
        corrections = json.load(f)
    
    # –ü–æ–º–µ—Ç–∏—Ç—å –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ
    for corr in corrections:
        if corr.get('marketplace') == marketplace:
            corr['used_for_training'] = True
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(corrections, f, ensure_ascii=False, indent=2)

def retrain_with_corrections(marketplace: str):
    """
    –ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å —É—á–µ—Ç–æ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    
    Args:
        marketplace: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞ (wildberries, ozon, yandex_market)
    """
    print(f"\n{'='*80}")
    print(f"üîÑ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø {marketplace.upper()}")
    print(f"{'='*80}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    corrections = load_corrections(marketplace)
    print(f"\nüìù –ù–∞–π–¥–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(corrections)}")
    
    if corrections:
        for corr in corrections[:5]:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 5
            print(f"  - {corr['product_name'][:50]}...")
            print(f"    –ë—ã–ª–æ: {corr['predicted_category']} ‚Üí –°—Ç–∞–ª–æ: {corr['corrected_category']}")
    
    # 2. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    BASE_DIR = Path(__file__).parent.parent
    PROJECT_ROOT = BASE_DIR.parent if BASE_DIR.name == 'src' else BASE_DIR
    dataset_path = PROJECT_ROOT / f'src/data/raw/{marketplace}_products_list.csv'
    
    if not dataset_path.exists():
        raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
    
    existing_df = pd.read_csv(dataset_path)
    print(f"\nüìä –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {len(existing_df)} —Ç–æ–≤–∞—Ä–æ–≤")
    
    # 3. –î–æ–±–∞–≤–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    corrections_df = add_corrections_to_dataset(corrections, marketplace)
    
    if len(corrections_df) > 0:
        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—ã
        # –ï—Å–ª–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å—Ç—å category_name, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if 'category_name' not in existing_df.columns and 'category_path' in existing_df.columns:
            existing_df['category_name'] = existing_df['category_path'].str.split('/').str[-1].str.strip()
        
        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å
        combined_df = pd.concat([existing_df, corrections_df], ignore_index=True)
        
        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ product_name (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π)
        combined_df = combined_df.drop_duplicates(subset=['product_name'], keep='last')
        
        print(f"‚úÖ –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(combined_df)} —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {len(corrections_df)}")
    else:
        combined_df = existing_df
        print("‚ö†Ô∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è")
    
    # 4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    temp_dataset = PROJECT_ROOT / f'src/data/raw/{marketplace}_with_corrections.csv'
    combined_df.to_csv(temp_dataset, index=False)
    
    # 5. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ
    from training.train_marketplace_models import MARKETPLACE_CONFIG
    config = MARKETPLACE_CONFIG[marketplace]
    
    print(f"\nüìä –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º category_name (–¥–æ—á–µ—Ä–Ω—è—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è)")
    
    X, y, vectorizer, to_id, to_label = preprocess_data(
        csv_file=str(temp_dataset),
        min_samples_per_category=config['min_samples'],
        category_column='category_name',
        max_features=config['max_features']
    )
    
    print(f"‚úÖ –ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"   X.shape: {X.shape}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(to_id)}")
    
    # 6. –û–±—É—á–µ–Ω–∏–µ
    y_cat = to_categorical(y)
    num_classes = y_cat.shape[1]
    
    model_dir = os.path.join(Config.MODELS_BIN, marketplace)
    os.makedirs(model_dir, exist_ok=True)
    
    save_preprocessing_objects(vectorizer, to_id, to_label, output_dir=model_dir)
    
    model = AutoencoderDL(
        input_dim=X.shape[1],
        bottleneck_dim=config['bottleneck_dim'],
        num_classes=num_classes
    )
    
    epochs = 50 if X.shape[0] < 30000 else 30
    
    print(f"\nüèãÔ∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    history = model.train_classifier(
        X, y_cat,
        epochs=epochs,
        batch_size=32,
        validation_split=0.2,
        use_early_stopping=True
    )
    
    # 7. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
    classifier_path = os.path.join(model_dir, 'classifier.h5')
    model.save(classifier_path)
    
    # 8. –ü–æ–º–µ—Ç–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ
    if corrections:
        mark_corrections_as_used(marketplace)
        print(f"\n‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–º–µ—á–µ–Ω—ã –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ")
    
    print(f"\n‚úÖ –ú–û–î–ï–õ–¨ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ê!")
    print(f"   –ü—É—Ç—å: {classifier_path}")
    print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {num_classes}")
    print(f"   –¢–æ–≤–∞—Ä–æ–≤: {X.shape[0]:,}")
    
    return model, history

if __name__ == '__main__':
    import sys
    
    marketplace = sys.argv[1] if len(sys.argv) > 1 else 'wildberries'
    retrain_with_corrections(marketplace)
